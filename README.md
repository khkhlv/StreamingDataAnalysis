# Потоковый анализ данных	

## Задание
**Лабораторная работа 1**

Сделать репозиторий, там ридми с ответами на вопросы:
1. Какие данные для этой области являются потоковыми?
2. Какие результаты мы хотим получить в результате обработки?
3. Как предметная область относится к запаздыванию обработки? Насколько это критично?
4. Как предметная область относится к потере данных? Насколько это критично? Какую семантику (не менее одного раза, не более одного раза, ровно один раз) следует выбрать?

Разобраться с генератором

**Лабораторная работа 2**
Развернуть кластер Apache Kafka. Написать ETL, который должен получать данные от источников и сохранять предобработанные данные в топики. При этом необходимо учитывать требования по семантикам, надежности и быстродействию, описанные в Лабораторной работе 1.

**Лабораторная работа 3**
Развернуть Apache Flink. Настроить чтение из топиков Kafka. Реализовать необходимые конвейеры для обработки потоков данных. Запись результатов - в новый топик

**Лабораторная работа 4**
Создать потоковое API (читает из топика с результатами и отдает клиентам). Создать потокового клиента. Выбрать паттерны и протоколы их взаимодействия.
Клиент – веб или мобильное приложение для конечного пользователя

## Описание работы

Предметная область: **Анализ котировок ценных бумаг Московской биржи**

Потоковые данные: *цена*, *объём*.

Результат обработки: статистика изменения цены и объёмов на различных таймфреймах.

Запаздывание обработки влияет на успех предсказания котировок, поэтому нужно добиваться наименьшего времени обработки.

Потеря данных критична, т.к. может исказить статистику и прогноз. Т.к. нам важна скорость, то выбираем семантику At Least Once. Возможное дублирование данных решается тем, что данные поступают в виде временных рядов.

Данные получаем при помощи Tinkoff Invest API в виде стриминга.

## Инструкция по запуску