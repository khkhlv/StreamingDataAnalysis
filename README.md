# Потоковый анализ данных	

## Задание
**Лабораторная работа 1**

Сделать репозиторий, там ридми с ответами на вопросы:
1. Какие данные для этой области являются потоковыми?
2. Какие результаты мы хотим получить в результате обработки?
3. Как предметная область относится к запаздыванию обработки? Насколько это критично?
4. Как предметная область относится к потере данных? Насколько это критично? Какую семантику (не менее одного раза, не более одного раза, ровно один раз) следует выбрать?

Разобраться с генератором

**Лабораторная работа 2**
Развернуть кластер Apache Kafka. Написать ETL, который должен получать данные от источников и сохранять предобработанные данные в топики. При этом необходимо учитывать требования по семантикам, надежности и быстродействию, описанные в Лабораторной работе 1.

**Лабораторная работа 3**
Развернуть Apache Flink. Настроить чтение из топиков Kafka. Реализовать необходимые конвейеры для обработки потоков данных. Запись результатов - в новый топик

**Лабораторная работа 4**
Создать потоковое API (читает из топика с результатами и отдает клиентам). Создать потокового клиента. Выбрать паттерны и протоколы их взаимодействия.
Клиент – веб или мобильное приложение для конечного пользователя

## Описание работы

Предметная область: **Анализ котировок ценных бумаг Московской биржи**

### 1. Какие данные для этой области являются потоковыми?

Потоковыми данными для Московской биржи являются:
- **Order book updates** — изменения стакана заявок (цена, объем, сторона сделки) в реальном времени
- **Trades** — исполненные сделки с указанием цены, объема, времени
- **Candles (OHLCV)** — агрегированные свечи (Open/High/Low/Close/Volume) с разной периодичностью (1 сек, 1 мин, 5 мин)
- **Market status events** — события о состоянии рынка (открытие/закрытие торгов)

### 2. Какие результаты мы хотим получить в результате обработки?

- **Технические индикаторы в реальном времени**: SMA, EMA, RSI, MACD, Bollinger Bands
- **Агрегированные метрики**: объемы по интервалам, волатильность, VWAP
- **Сигналы для трейдинга**: пересечение скользящих средних, уровни перекупленности/перепроданности
- **Аномалии**: резкие движения цены (>2% за 10 сек), необычные объемы
- **Статистика ликвидности**: спред стакана, глубина рынка

### 3. Как предметная область относится к запаздыванию обработки? Насколько это критично?

Зависит от сценария использования:

| Сценарий                 | Допустимая задержка | Критичность |
|--------------------------|---------------------|-------------|
| Высокочастотный трейдинг | < 10 мс             | Критично    |
| Алгоритмический трейдинг | 10–100 мс           | Высокая     |
| Торговля                 | 100 мс – 1 сек      | Средняя     |
| Аналитика/визуализация   | 1–5 сек             | Низкая      |

### 4. Как предметная область относится к потере данных? Насколько это критично? Какую семантику следует выбрать?

**Потеря данных недопустима** для:
- Расчета финансовых показателей (неверный итог = финансовые потери)
- Юридической отчетности и аудита
- Воспроизведения торговой сессии для анализа

**Семантика доставки:**
- **Exactly-once (ровно один раз)** — для аналитики
- **At-least-once (не менее одного раза)** + идемпотентная обработка — для трейдинга

В текущей архитектуре:
- Kafka: `enable.idempotence=true`, `acks=all`, `min.insync.replicas=2`
- Flink: checkpointing с интервалом 5–10 сек + двухфазный коммит в Kafka
- Producer: идемпотентные операции с ключами на основе (ticker, timestamp)

## Инструкция к запуску

# Шаг 1: Запуск инфраструктуры
docker compose up -d

# Шаг 2: Отправка Flink job (Терминал 1)
./flink/submit-job.sh

# Шаг 3: Запуск генератора (Терминал 2)
python3 generator/kafka_producer.py

# Шаг 4: Запуск клиента (Терминал 3)
uvicorn client.app:app --reload

# Шаг 5: Мониторинг
# Веб-интерфейс Flink: http://localhost:8081
# Веб-клиент: http://localhost:8000/
